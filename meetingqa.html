
<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Improving Vision-and-Language Navigation by Generating Future-View Image Semantics">
  <meta name="keywords" content="VLN">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Improving Vision-and-Language Navigation by Generating Future-View Image Semantics</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script> -->
  <!-- <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./prj_static/css/bulma.min.css">
  <link rel="stylesheet" href="./prj_static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./prj_static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./prj_static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./prj_static/css/index.css">
  <!-- <link rel="icon" href="./prj_static/images/favicon.svg"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./prj_static/js/fontawesome.all.min.js"></script>
  <script src="./prj_static/js/bulma-carousel.min.js"></script>
  <script src="./prj_static/js/bulma-slider.min.js"></script>
  <script src="./prj_static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://jialuli-luka.github.io/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          Projects
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://jialuli-luka.github.io/VLN-SIG">
            VLN-SIG
          </a>
          <a class="navbar-item" href="https://arxiv.org/abs/2203.15685">
            EnvEdit
          </a>
          <a class="navbar-item" href="https://arxiv.org/abs/2207.02185">
            CLEAR
          </a>
          <a class="navbar-item" href="https://aclanthology.org/2021.naacl-main.82/">
            SyntaxVLN
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-3 publication-title">Improving Vision-and-Language Navigation by Generating Future-View Image Semantics (CVPR 2023)</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://jialuli-luka.github.io/">Jialu Li</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://www.cs.unc.edu/~mbansal/">Mohit Bansal</a><sup>1</sup>,</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of North Carolina, Chapel Hill</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2304.04907"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/jialuli-luka/VLN-SIG"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">

      <center><img src="./VLN-SIG/intro.png" alt="Teaser" width="70%"></center>

      <div class="content has-text-justified">
        Overview of our proposed method VLN-SIG. We obtain the semantics of an image with a pre-trained image tokenizer, and use codebook selection and patch semantic calculation to adapt it to efficient in-domain VLN learning. We pre train the agent on three proxy tasks and fine-tune the agent using Action Prediction with Image Generation (APIG) as the additional auxiliary task.
      </div>

    </div>
  </div>
</section>

<hr>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Vision-and-Language Navigation (VLN) is the task that requires an agent to navigate through the environment based on natural language instructions.
   At each step, the agent takes the next action by selecting from a set of navigable locations. In this paper, we aim to take one step further and explore whether the agent can benefit from generating the potential future view during navigation. Intuitively, humans will have an expectation of how the future environment will look like, based on the natural language instructions and surrounding views, which will aid correct navigation. Hence, to equip the agent with this ability to generate the semantics of future navigation views, we first propose three proxy tasks during the agent's in-domain pre-training: Masked Panorama Modeling (MPM), Masked Trajectory Modeling (MTM), and Action Prediction with Image Generation (APIG). These three objectives teach the model to predict missing views in a panorama (MPM), predict missing steps in the full trajectory (MTM), and generate the next view based on the full instruction and navigation history (APIG), respectively.
We then fine-tune the agent on the VLN task with an auxiliary loss that minimizes the difference between the view semantics generated by the agent and the ground truth view semantics of the next step. Empirically, our VLN-SIG achieves the new state-of-the-art on both Room-to-Room dataset and CVDN dataset. We further show that our agent learns to fill in missing patches in future views qualitatively, which brings more interpretability over agents' predicted actions. Lastly, we demonstrate that learning to predict future view semantics also enables the agent to have better performance on longer paths.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

</section>
<hr>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
    <h2 class="title is-3">Method</h2>

      <center><img src="./VLN-SIG/method.png" alt="Teaser" width="100%"></center>

      <div class="content has-text-justified">
<p>We first calculate the overall image semantics with a weight function after filtering the visual vocabulary with dynamic codebook selection method. Then, we pre-train the VLN agent with three proxy tasks to learn the image semantic information.
  Specifically, Masked Trajectory Modeling predicts the missing image semantics in a given navigation trajectory based on instructions. Masked Panorama Modeling predicts the image semantics of the missing views in a panorama. MTM and MPM together help the agent learn image semantics in both temporal and spatial space.
Action Prediction with Image Generation mimics the action prediction process and predicts the image semantics of the next step. Lastly, we fine-tune the navigation agent on navigation loss and APIG loss. </p>
      </div>

    </div>
  </div>
  </div>
</section>
<hr>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
    <h2 class="title is-3">Results</h2>
    <h2 class="title is-5">Test Leaderboard Performance</h2>
    <center>
    <table border="1" >
  <tr>
    <th style="text-align: center; vertical-align: middle;" colspan="1">Model</th>
    <th style="text-align: center; vertical-align: middle;" colspan="4">R2R</th>
    <th style="text-align: center; vertical-align: middle;" colspan="2">CVDN</th>
  </tr>
  <tr>
    <td style="text-align: center; vertical-align: middle;" colspan="1"></td>
    <td style="text-align: center; vertical-align: middle;" colspan="2">Val Unseen</td>
    <td style="text-align: center; vertical-align: middle;" colspan="2">Test</td>
    <td style="text-align: center; vertical-align: middle;" colspan="1">Val Unseen</td>
    <td style="text-align: center; vertical-align: middle;" colspan="1">Test</td>
  </tr>
  <tr>
    <td></td>
    <td style="text-align: center; vertical-align: middle;">SR</td>
    <td style="text-align: center; vertical-align: middle;">SPL</td>
    <td style="text-align: center; vertical-align: middle;">SR</td>
    <td style="text-align: center; vertical-align: middle;">SPL</td>
    <td style="text-align: center; vertical-align: middle;">GP</td>
    <td style="text-align: center; vertical-align: middle;">GP</td>
  </tr>
  <tr>
    <td style="text-align: center; vertical-align: middle; width: 20%"><a href="https://arxiv.org/abs/2002.10638">PREVALENT</a></td>
    <td style="text-align: center; vertical-align: middle; width: 10%">58</td>
    <td style="text-align: center; vertical-align: middle; width: 10%">53</td>
    <td style="text-align: center; vertical-align: middle; width: 10%">54</td>
    <td style="text-align: center; vertical-align: middle; width: 10%">51</td>
    <td style="text-align: center; vertical-align: middle; width: 10%">3.15</td>
    <td style="text-align: center; vertical-align: middle; width: 10%">2.44</td>
  </tr>
  <tr>
    <td style="text-align: center; vertical-align: middle; width: 20%"><a href="https://arxiv.org/abs/2011.13922">Rec-BERT</a></td>
    <td style="text-align: center; vertical-align: middle; width: 10%">63</td>
    <td style="text-align: center; vertical-align: middle; width: 10%">57</td>
    <td style="text-align: center; vertical-align: middle; width: 10%">63</td>
    <td style="text-align: center; vertical-align: middle; width: 10%">57</td>
    <td style="text-align: center; vertical-align: middle; width: 10%">-</td>
    <td style="text-align: center; vertical-align: middle; width: 10%">-</td>
  </tr>
  <tr>
    <td style="text-align: center; vertical-align: middle; width: 20%"><a href="https://arxiv.org/abs/2110.13309">HAMT</a></td>
    <td style="text-align: center; vertical-align: middle; width: 10%">66</td>
    <td style="text-align: center; vertical-align: middle; width: 10%">61</td>
    <td style="text-align: center; vertical-align: middle; width: 10%">65</td>
    <td style="text-align: center; vertical-align: middle; width: 10%">60</td>
    <td style="text-align: center; vertical-align: middle; width: 10%">5.13</td>
    <td style="text-align: center; vertical-align: middle; width: 10%">5.58</td>
  </tr>
  <tr>
    <td style="text-align: center; vertical-align: middle; width: 20%">Ours</td>
    <td style="text-align: center; vertical-align: middle; width: 10%"><b>68</b></td>
    <td style="text-align: center; vertical-align: middle; width: 10%"><b>62</b></td>
    <td style="text-align: center; vertical-align: middle; width: 10%"><b>65</b></td>
    <td style="text-align: center; vertical-align: middle; width: 10%"><b>60</b></td>
    <td style="text-align: center; vertical-align: middle; width: 10%"><b>5.52</b></td>
    <td style="text-align: center; vertical-align: middle; width: 10%"><b>5.83</b></td>
  </tr>
  <tr>
    <td style="text-align: center; vertical-align: middle; width: 20%"><a href="https://arxiv.org/abs/2202.11742">DUET<sup>*</sup></a></td>
    <td style="text-align: center; vertical-align: middle; width: 10%">72</td>
    <td style="text-align: center; vertical-align: middle; width: 10%">60</td>
    <td style="text-align: center; vertical-align: middle; width: 10%">69</td>
    <td style="text-align: center; vertical-align: middle; width: 10%">59</td>
    <td style="text-align: center; vertical-align: middle; width: 10%">-</td>
    <td style="text-align: center; vertical-align: middle; width: 10%">-</td>
  </tr>
  <tr>
    <td style="text-align: center; vertical-align: middle; width: 20%">Ours<sup>*</sup></td>
    <td style="text-align: center; vertical-align: middle; width: 10%"><b>72</b></td>
    <td style="text-align: center; vertical-align: middle; width: 10%"><b>62</b></td>
    <td style="text-align: center; vertical-align: middle; width: 10%"><b>72</b></td>
    <td style="text-align: center; vertical-align: middle; width: 10%"><b>60</b></td>
    <td style="text-align: center; vertical-align: middle; width: 10%"><b>-</b></td>
    <td style="text-align: center; vertical-align: middle; width: 10%"><b>-</b></td>
  </tr>
</table>
</center>
      <div class="content has-text-justified">
<p>Comparison with state-of-the-art agents on Room-to-Room (R2R) and Cooperative Vision-and-Dialog Navigation (CVDN) validation unseen set and test leaderboard. * denotes agent that utilizes graph information during navigation.</p>
      </div>

    </div>
  </div>
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-5">Ablation Results -- Effectiveness of Pre-training Tasks</h2>

    <center><img src="./VLN-SIG/table1.png" alt="Teaser" width="100%"></center>

    <div class="content has-text-justified">
      We demonstrate the effectiveness of our proposed three tasks for VLN in-domain pre-training. Adding any of our proposed tasks improves the performance by a large margin in both success rate (SR) and success rate weighted by path length (SPL).
    </div>
  </div>
  </div>

  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-5">Ablation Results -- Effectiveness of Dynamic Codebook Selection</h2>

    <center><img src="./VLN-SIG/table2.png" alt="Teaser" width="100%"></center>

    <div class="content has-text-justified">
      We demonstrate that it’s important to select a subset of visual tokens either statically or dynamically for the agent to effectively learn the large visual token vocabulary extracted with pre-trained dVAE.
    </div>
  </div>
  </div>

  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-5">Ablation Results -- Comparison of Image Semantic Representation Methods</h2>

    <center><img src="./VLN-SIG/table3.png" alt="Teaser" width="100%"></center>

    <div class="content has-text-justified">
      We also show that block-wise weighted function help the agent to learn the image semantics better.
    </div>
  </div>
  </div>
  </div>
</section>
<hr>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
    <h2 class="title is-3">Generation Examples</h2>

    <div class="result_row">
      <div class="result_column">
        <!-- <iframe src="./VLN-SIG/example1.pdf" width="100%" height="300px" style="border: 0;"></iframe> -->
  <img src="./VLN-SIG/example1.png" alt="Teaser">
</div>
<div class="result_column">
  <img src="./VLN-SIG/example2.png" alt="Teaser">
</div>
</div>

      <div class="content has-text-justified">
<p>Our model could reasonably generate future semantics and reconstruct future images. We generate the token for each patch in the image with our APIG head learned with weighted patch probability. We set the weight vector to be an indicator vector, where the weight for the target patch is 1 and the others are 0. We use x% (x&#8712{70, 80, 90}) of the patches with ground truth tokens encoded with pre-trained dVAE, the rest patches with the tokens generated by our APIG head, and then use the dVAE decoder to decode the image. Our generated image could almost reconstruct the beds and pictures in the original image with small vague areas when given 70% of the ground truth tokens. In comparison, filling the 30% patches with random tokens will generate distorted images with large white parts, and the beds and pictures information cannot be identified. We also notice that our model still fails to generate the full image when all the tokens in the image are predicted by our APIG head, and needs at least 70% of the ground truth tokens to generate images of reasonable quality.
 </p>
      </div>

      <div class="content has-text-justified">
We furture compare the generated semantics with the ground truth semantics quantitatively to demonstrate that the semantic information underlying them is similar. Specifically, we represent the semantics of each visual token as the output of the first embedding layer in the dVAE decoder (which maps each token to a 128 dimension representation space). We calculate the distance between generated semantics and ground truth semantics, and compare it with the distance between the ground truth semantics and all other tokens in the vocabulary (i.e., the distance between the ground truth token and other 8191 tokens for each patch). We normalize each semantic representation and use l2-norm as the distance. Our method has a distance of 0.95, while the baseline is 1.31. This shows that the distance between our generated semantics and ground truth semantics is closer.
      </div>

    </div>
  </div>
  </div>
</section>
<hr>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{li2023vln-sig,
  author    = {Jialu Li, and Mohit Bansal},
  title     = {Improving Vision-and-Language Navigation by Generating Future-View Image Semantics},
  journal   = {CVPR},
  year      = {2023},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p style="text-align:right;font-size:small;">
          <a href="https://github.com/nerfies/nerfies.github.io">
            This guy makes a nice webpage.
          </a>
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>