
<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="MeetingQA: Extractive Question-Answering on Meeting Transcripts">
  <meta name="keywords" content="MeetingQA">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MeetingQA: Extractive Question-Answering on Meeting Transcripts &ndash; Project Page</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script> -->
  <!-- <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./prj_static/css/bulma.min.css">
  <link rel="stylesheet" href="./prj_static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./prj_static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./prj_static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./prj_static/css/index.css">
  <!-- <link rel="icon" href="./prj_static/images/favicon.svg"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./prj_static/js/fontawesome.all.min.js"></script>
  <script src="./prj_static/js/bulma-carousel.min.js"></script>
  <script src="./prj_static/js/bulma-slider.min.js"></script>
  <script src="./prj_static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://archiki.github.io/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          Projects
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://archiki.github.io/MeetingQA.html">
            MeetingQA
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-3 publication-title">MeetingQA: Extractive Question-Answering on Meeting Transcripts (ACL 2023)</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://archiki.github.io/">Archiki Prasad</a><sup>1</sup>,</span>
            <span class="author-block">
            <a href="https://research.adobe.com/person/trung-bui/">Trung Bui</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://david-yoon.github.io/">Seunghyun Yoon</a><sup>2</sup>,</span>
              <a href="https://research.adobe.com/person/hanieh-deilamsalehy/">Hanieh Deilamsalehy</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://research.adobe.com/person/franck-dernoncourt/">Franck Dernoncourt</a><sup>2</sup>,</span>
              <a href="https://www.cs.unc.edu/~mbansal/">Mohit Bansal</a><sup>1</sup>,</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of North Carolina, Chapel Hill</span>
            <span class="author-block"><sup>2</sup>Adobe Research</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://archiki.github.io/files/MeetingQA_ACL2023.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://drive.google.com/file/d/1DW4ECOoaGli_o-a7Tg3PJiWgudCglCZJ/view?usp=sharing"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/adobe-research/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://github.com/adobe-research/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
                <!-- Poster Link. -->
              <span class="link-block">
                <a href="https://archiki.github.io/files/Poster_4357.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Poster</span>
                  <!-- Slides Link. -->
              <span class="link-block">
                <a href="https://archiki.github.io/files/MeetingQA-Slides.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Slides</span>
                </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
    <div class="container is-max-desktop">
        <div class="column has-text-centered">
            <div class="column">
              <img src="./files/UNCnlp.jpeg" alt="UNC NLP" style="width:30%">
            </div>
            <div class="column">
              <img src="./files/adobe.jpg" alt="Adobe Research" style="width:20%">
            </div>
          </div>
        </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">

      <left><img src="./files/images/Fig1.png" alt="Teaser" width="60%"></left>

      <div class="content has-text-justified">
        Overview of our new and challenging extractive question answering (QA) dataset: MeetingQA. We annotate public meetings from the AMI corpus for QA (100 hours) containing a total of 7.7K questions. Our questions are longer,open-ended, and discussion-seeking including interesting scenarios such as rhetorical questions, multi-span answers and/or multi-speaker answers. Compared to human performance, fine-tuned models (single/multi-span, and short/long-context variants) severely underperform leaving huge room for improvement (~25 F1 points). In the zero-shot setting, the performance gap widens even further (~45 F1 points).


      </div>

    </div>
  </div>
</section>

<hr>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            With the ubiquitous use of online meeting platforms and robust automatic speech recognition systems, meeting transcripts have emerged as a new and interesting domain for natural language tasks. Most recent works on meeting transcripts are restricted to summarization and extraction of action items. However, meeting discussions also have a useful question-answering (QA) component, crucial to understanding the discourse or meeting content, and can be used to build interactive interfaces on top of long transcripts. Hence, in this work, we leverage this inherent QA component of meeting discussions and introduce MeetingQA, an extractive QA dataset comprising questions asked by meeting participants and corresponding responses. As a result, questions can be open-ended and seek active discussions, while the answers can be multi-span and spread across multiple speakers. Our comprehensive empirical study of several robust baselines including long-context language models and recent instruction-tuned models reveals that models perform poorly on this task (F1 = 57.3) and severely lag behind human performance (F1 = 84.6), thus presenting a useful, challenging new task for the community to improve upon.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

</section>
<hr>

<section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
      <h2 class="title is-3">Data Collection and Analysis</h2>
  
        <center><img src="./files/images/Tab1.png" alt="Teaser" width="70%"></center>
        <center><img src="./files/images/human_eval.png" alt="Teaser" width="100%"></center>
  
        <div class="content has-text-justified">
  <p>
    We annotated public meetings from AMI (Augmented Multi-party Interaction) corpus with ~100 hours manually transcribed meetings.
To this end, we recruited annotators to label which sentences from the transcript answer each question along with meta-data.
We found high inter-annotator agreement with Krippendorff’s α of 0.73, obtaining annotations for 166 meetings at $61 per meeting. 
    </p>

<p>
<strong>Question Types</strong>: Even questions framed in ‘yes/no’ manner are information-seeking and elicit detailed responses, ~50% of questions are opinion-seeking and ~20% are framed rhetorically.<br>
<strong>Answer Types</strong>: 30% of questions are unanswerable, 40% of answers are multi-span (non-consecutive sentences) and 48% involve multiple speakers. Nearly 70% of multi-speaker answers contain some level of disagreement among participants.<br>
<strong>Length Distribution</strong>: Average length of a transcript, question, and corresponding answer is 5.9K, 12, and 35 words, respectively.<br>
<strong>Human Performance</strong>: F1=84.6 on 250 questions from the test set.<br>

    </p>
        </div>
  
      </div>
    </div>
    </div>
  </section>
  <hr>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
    <h2 class="title is-3">Method</h2>

      <div class="content has-text-justified">
<p>
    For short-context models, we find that the entire meeting transcripts do not fit in the input context. Thus, we retrieve a segment from transcript based on location of the question. Long-context models on the other hand have longer input context budgets, so for these we fit as much of the transcript as possible (around the question). We explore both single-span models that predict a single-span answer from first to last relevant sentence and multi-span models that treat QA as token-classification task. Additionally, we augment training data with automatically annotated answer spans for interviews from MediaSum dataset.

    
</p>
      </div>

    </div>
  </div>
  </div>
</section>
<hr>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
    <h2 class="title is-3">Results</h2>
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-5">Performance: Fine-tuned</h2>

    <center><img src="./files/images/Tab3.png" alt="Teaser" width="100%"></center>

    <div class="content has-text-justified">
        We find that short-context models slightly outperform long-context models by 1-2 F1 points. Additionally, multi-span models have comparable or less performance than single-span models.
        In summary, we notive ≥ 25 F1 points gap with human performance.
        
        
    </div>
  </div>
  </div>

  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-5">Performance: Zero-shot</h2>

    <center><img src="./files/images/Tab4.png" alt="Teaser" width="70%"></center>

    <div class="content has-text-justified">
        We observe that all models exhibit poor zero-shot performance (~45 F1 points gap). Furthermore, augmenting with silver data improves zero-shot performance. We also demostrate that larger instruction-tuned LMs (Flan-T5) yield comparable performance.
        
    </div>
  </div>
  </div>

  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-5">Error-Analysis</h2>

    <center><img src="./files/images/Fig4.png" alt="Teaser" width="70%"></center>

    <div class="content has-text-justified">
    Models struggle at identifying rhetorical questions, especially in zero-shot settings. Further, single-span predictions contain a greater fraction of irrelevant sentences. Lastly, models struggle to identify which speakers answer a question, especially in zero-shot setting.
        
    </div>
  </div>
  </div>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{prasad2023meeting,
  author    = {Archiki Prasad, Trung Bui, Seunghyun Yoon,  Hanieh Deilamsalehy, Franck Dernoncourt, and Mohit Bansal},
  title     = {MeetingQA: Extractive Question-Answering on Meeting Transcripts},
  journal   = {ACL},
  year      = {2023},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p style="text-align:right;font-size:small;">
          <a href="https://github.com/nerfies/nerfies.github.io">
            This guy makes a nice webpage.
          </a>
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>