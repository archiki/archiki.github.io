<!DOCTYPE html>
<html>
  <head>
    <title>Archiki Prasad</title>
    <base target="_blank">
        <meta charset="utf-8" />
    <meta content='text/html; charset=utf-8' http-equiv='Content-Type'>
    <meta http-equiv='X-UA-Compatible' content='IE=edge'>
    <meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>

    
    <!-- <meta name="description" content="Electrical Engineering Dual-Degree Student, IIT Bombay">
    <meta property="og:description" content="Electrical Engineering Dual-Degree Student, IIT Bombay" /> -->
    
    <meta name="author" content="Archiki Prasad" />

    

    <!--[if lt IE 9]>
      <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link href="https://fonts.googleapis.com/css?family=Roboto+Slab:300,400,700" rel="stylesheet">
    <link rel="alternate" type="application/rss+xml" title="Archiki Prasad - Electrical Engineering Dual-Degree Student, IIT Bombay" href="/feed.xml" />
    <link rel="icon" type="image/png" href="./files/iitb.png">
    <!-- Created with Jekyll Now - http://github.com/barryclark/jekyll-now -->
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
        <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>
  </head>

  <body>
    <div class="wrapper-masthead">
      <div class="container">
        <header class="masthead clearfix">
          
          <div class="site-info">
            <h1 class="site-name"><a href="/">Archiki Prasad</a></h1>
            <!-- <p class="site-description">Electrical Engineering Dual-Degree Student, IIT Bombay</p> -->
          </div>

          <nav>
            <a href="/about" target="_self">About</a>
            <a href="/research" target="_self">Research</a>
            <a href="/files/Resume.pdf">Resume</a>
            <a href="/files/CV.pdf">CV</a>
          </nav>
        </header>
      </div>
    </div>

<!-- <div id="archiki_acl20_bib" class="entry" style="display:none;">  -->
                           <!-- </br>
<pre>
@inproceedings{prasad2020accents,
  title={How Accents Confound: Probing for Accent Information in End-to-End Speech Recognition Systems},
  author={Prasad, Archiki and Jyothi, Preethi},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={3739--3753},
  year={2020}
}
}
</pre>
</div> -->
    <div id="main" role="main" class="container">
      <article class="post">
  <h1>Research</h1>

  <div class="entry">
    <p>My research interests broadly lie in the fields of Machine Learning, Natural Language Processing and Speech Processing. In the past, I have worked on time-series and sequence-to-sequence tasks. I am interested in explainability and interpretability of black-box models in Natural Language Processing and Automatic Speech Recognition. I am also interested in building machine learning models that robust to distributional shifts and are not dependent on dataset biases or spurious correlations. Recently, I have been fascinated by problems around context in converstational and dialogue systems. </p>

<h3 id="publications">PUBLICATIONS & PATENTS</h3>

<ul>
  <li>
    <p><b><a href="https://www.aclweb.org/anthology/2020.acl-main.345.pdf">How Accents Confound: Probing for Accent Information in End-to-End Speech Recognition Systems</a></b><br />
 <strong>Archiki Prasad</strong> and Preethi Jyothi<br />
 <b><a href="https://www.aclweb.org/anthology/volumes/2020.acl-main/">ACL 2020</a></b><br /> 
<!-- [<a href="#archiki_acl20_bib">bib</a>] -->
[<a href="#" onclick="$('#archiki_acl20_abstract').toggle();return false;" style="font-variant: small-caps;">abstract</a>]                          
[<a href="https://github.com/archiki/ASR-Accent-Analysis" style="font-variant: small-caps;">code</a>] [<a href="https://slideslive.com/38929438/how-accents-confound-probing-for-accent-information-in-endtoend-speech-recognition-systems" style="font-variant: small-caps;">talk</a>]
<div id="archiki_acl20_abstract" class="abstract" style="display:none;">
</br>
    <p>
        In this work, we present a detailed analysis of how accent information is reflected in the internal representation of speech in an end-to-end automatic speech recognition (ASR) system. We use a state-of-the-art end-to-end ASR system, comprising convolutional and recurrent layers, that is trained on a large amount of US-accented English speech and evaluate the model on speech samples from seven different English accents. We examine the effects of accent on the internal representation using three main probing techniques: a) Gradient-based explanation methods, b) Information-theoretic measures, and c) Outputs of accent and phone classifiers. We find different accents exhibiting similar trends irrespective of the probing technique used. We also find that most accent information is encoded within the first recurrent layer, which is suggestive of how one could adapt such an end-to-end model to learn representations that are invariant to accents.
    </p>
</div>
</p>

  </li>
  <li>
    <p><b><a href="https://dl.acm.org/doi/pdf/10.1145/3366424.3382728">Time Series Forecasting for Cold-Start Items by Learning from Related Items using Memory Networks</a></b><br />
Ayush Chauhan, <strong>Archiki Prasad</strong>, Parth Gupta, Amiredddy Prashanth Reddy and Shiv Kumar Saini<br />
<b><a href="https://www2020.thewebconf.org/">The Web Conference (WWW) 2020 </a></b><br /> 
[<a href="#" onclick="$('#archiki_www20_abstract').toggle();return false;" style="font-variant: small-caps;">abstract</a>]
<div id="archiki_www20_abstract" class="abstract" style="display:none;">
</br>
    <p>
        Time series forecasting for new items is very important in a wide variety of applications. Existing solutions for time series forecasting, however, do not address this cold start problem. The underlying machine learning models in these solutions rely heavily on the availability of the past data points of the time series. Here, we propose to use a modified Dynamic Key-Value Memory Network (DKVMN) that enables knowledge sharing across items. The network is conventionally used for binary tasks in knowledge tracing. We modify it for our regression-based forecasting use-case. Specifically, we change the output layer, include feedback for error correction, add a mechanism to handle scale across items. We test our solution on the SKU level data of a large e-commerce company and compare the results to the widely used LSTM model, outperforming it by over 25% across multiple metrics.
    </p>
</div>
</p>
  </li>
  <li>
    <p><b>Key-Value Memory Networks for Predicting Time Series Metrics of Target Entities</b><br />
Shiv Kumar Saini, Ayush Chauhan, Parth Gupta, <strong>Archiki Prasad</strong>, Ritwick Chaudhary and Amiredddy Prashanth Reddy<br />
<em>Patent filed at the </em><b><a href="https://www.uspto.gov/">US Patent and Trademarks Office</a></b> <em>2020 | Adobe Inc.</em><br />
[<a href="#" onclick="$('#archiki_patent').toggle();return false;" style="font-variant: small-caps;">summary</a>] [Application No. US16/868942]
<div id="archiki_patent" class="abstract" style="display:none;">
</br>
    <p>
        This disclosure involves using key-value memory networks to predict time-series data. For instance, a computing system retrieves, for a target entity, static feature data and target time-series feature data. The computing system can normalize the target time-series feature data based on a normalization scale. The computing system also generates input data by, for example, concatenating the static feature data, the normalized time-series feature data, and time-specific feature data. The computing system generates predicted time-series data for the target metric of the target entity by applying a key-value memory network to the input data. The key-value memory network can include a key matrix learned from training static feature data and training time-series feature data, a value matrix representing time-series trends, and an output layer with a continuous activation function for generating predicted time-series data.
    </p>
</div>
</p>
  </li>
  <li>
  <p><b><a href="https://arxiv.org/pdf/2009.12961.pdf">Decentralized Age-of-Information Bandits</a></b> <em> (Pre-Print) </em><br />
<strong>Archiki Prasad</strong>, Vishal Jain and Sharayu Moharir<br />
<em>Submitted to </em><b><a href="https://wcnc2021.ieee-wcnc.org/">IEEE-WCNC 2021</a></b><br /> 
[<a href="#" onclick="$('#archiki_wcnc21_abstract').toggle();return false;" style="font-variant: small-caps;">abstract</a>] [Submission available on request]
<div id="archiki_wcnc21_abstract" class="abstract" style="display:none;">
</br>
    <p>
        Age-of-Information (AoI) is a performance metric for scheduling systems that measures the freshness of the data available at the intended destination. AoI is formally defined as the time elapsed since the destination received the recent most update from the source. We consider the problem of scheduling to minimize the cumulative AoI in a multi-source multi-channel setting. Our focus is on the setting where channel statistics are unknown and we model the problem as a distributed multi-armed bandit problem. For an appropriately defined AoI regret metric, we provide analytical performance guarantees of an existing UCB-based policy for the distributed multi-armed bandit problem. In addition, we propose a novel policy based on Thomson Sampling and a hybrid policy that tries to balance the trade-off between the aforementioned policies. Further, we develop AoI-aware variants of these policies in which each source takes its current AoI into account while making decisions. We compare the performance of various policies via simulations. 
    </p>
</div>
</p>
  </li>
  <li>
  <p><b><a href="https://archiki.github.io/files/ICASSP.pdf">An Investigation of End-to-End Models for Robust Speech Recognition</a></b> <em> (Pre-Print) </em><br />
<strong>Archiki Prasad</strong>, Preethi Jyothi and Rajbabu Velmurugan<br />
<em>Submitted to </em><b><a href="https://2021.ieeeicassp.org/">IEEE-ICASSP 2021</a></b><br /> 
[<a href="#" onclick="$('#archiki_icassp21_abstract').toggle();return false;" style="font-variant: small-caps;">abstract</a>] 
<div id="archiki_icassp21_abstract" class="abstract" style="display:none;">
</br>
    <p>
        End-to-end models for robust automatic speech recognition (ASR) have not been sufficiently well-explored in prior work. With end-to-end models, one could choose to preprocess the input speech using speech enhancement techniques and train the model using enhanced speech. Another alternative is to pass the noisy speech as input and modify the model architecture to adapt to noisy speech. A systematic comparison of these two approaches for end-to-end robust ASR has not been attempted before. We address this gap and present a detailed comparison of speech enhancement-based techniques and three different model-based adaptation techniques covering data augmentation, multi-task learning, and adversarial learning for robust ASR. While adversarial learning is the best-performing technique on certain noise types, it comes at a cost of degrading clean speech WER. On other relatively stationary noise types, a new speech enhancement technique outperformed all the model-based daptation techniques. This suggests that knowledge of the underlying noise type can meaningfully inform the choice of adaptation technique.
            </p>
</div>
</p>
  </li>
</ul>

<!-- <p><sup>†</sup><em>Equal Contribution</em></p> -->

<h3 id="research-projects">RESEARCH PROJECTS</h3>
Please have a look at my <em><a href="/files/CV.pdf"><strong>Curriculum Vitae</strong></a></em> for a comprehensive list of my research projects and experience.

<!-- <h3 id="2019">2019</h3> -->

<!-- <p><strong>Learning with Noisy Sequence Labels</strong><br />
<em>Bachelor’s Thesis guided by <a href="https://www.cse.iitb.ac.in/~pjyothi/">Prof. Preethi Jyothi</a></em></p>

<p>Majority of the datasets collected by crowdsourcing tend to have noisy labels. Although there is a lot of theoretical and empirical work for learning with noisy labels for multiclass classification, there is very sparse work for structured outputs such as sequences. We are currently working on the problem of probablistic label aggregation for tasking involving sequences such as Part of Speech tagging and Named Entity Recognition.</p>

<p><strong>Policy Iteration Lower Bounds for Multi-Action MDPs</strong> <a href="https://joshinh.github.io/files/CS747_report.pdf">[report]</a><br />
<em>Guide: <a href="https://www.cse.iitb.ac.in/~shivaram/">Prof. Shivaram K</a> (Course Project)</em></p>

<p>Simple Policy Iteration is a type of policy iteration algorithm where the policy of only one improvable state is changed at every step. <a href="https://pdfs.semanticscholar.org/b321/9edc2ce55b2d7f5d45cc014a0d2733ed3051.pdf">Melekopoglou and Condon</a> showed an exponential lower bound for 2-action MDPs. We generalized the MDPs to k-actions and demonstrate a lower bound of O(k.exp(n)).</p>

<p><strong>Explainable Natural Language Inference</strong><br />
<em>Guide: <a href="http://www.nec-labs.com/christopher-malon">Dr. Christopher Malon</a> (Internship at NEC Labs, Princeton)</em></p>

<p>Simple entailment models try to judge the hypotheses as true, false, or unsupported based on information in a single sentence or group of concatenated sentences, but this information is sometimes insufficient. We constructed datasets for multi-hop NLI by transforming existing multi-hop QA datasets and proposed models which could perform multi-hop reasoning. Our model could also generate explanations for the inferential relationship without any direct supervision.</p>

<h3 id="2018">2018</h3>

<p><strong>Interpretable Multi-hop Reading Comprehension</strong> <a href="https://www.aclweb.org/anthology/P19-1261/">[paper]</a> <br />
<em>Guide: <a href="">Prof. Mohit Bansal</a> (Internship at UNC Chapel Hill)</em></p>

<p>Built interpretable models for multi-hop reading comprehension which searched and assembled important contextual information, by constructing a tree of reasoning chains to answer a given question. By constructing explicit reasoning chains, it was possible to understand the exact evidence used by the model. Achieved strong results on two challenging datasets and also demonstrated the interpretability through various analysis and ablations.</p>

<p><strong>Cross-lingual Question Generation</strong> <a href="https://www.aclweb.org/anthology/P19-1481/">[paper]</a><br />
<em>Guide: <a href="https://www.cse.iitb.ac.in/~ganesh/">Prof. Ganesh Ramakrishnan</a> and <a href="https://www.cse.iitb.ac.in/~pjyothi/">Prof. Preethi Jyothi</a></em></p>

<p>Many Natural Language Generation tasks assume access to large supervised datasets which may not be available for all languages. We proposed cross-lingual pre-training methods which could leverage information from data rich languages (such as English) to improve performance on the downstream task for low resource languages (such as Hindi). We also released a small Hindi QA dataset along with the work.</p>

<p><strong>Accent Adaptation for Speech Recognition</strong> <a href="https://joshinh.github.io/files/RnD_Report.pdf">[report]</a> <br />
<em>Guide: <a href="https://www.cse.iitb.ac.in/~pjyothi/">Prof. Preethi Jyothi</a></em></p>

<p>Majority of the state-of-the-art automatic speech recognition (ASR) systems for English are trained on the readily available US-accented data and perform poorly on other accents. We introduced domain adversarial training techniques to adapt ASR systems to low resource accents. Extended this work (later in 2019) by proposing a novel coupled training paradigm which exploited speech data with same text content. Our method gave huge gains in performance on heavy accents such as Indian.</p> -->

  </div>

  <!--div class="date">
    Written on 
  </div
//-->

  
</article>
</div>

    <div class="wrapper-footer">
      <div class="container">
        <footer class="footer">
<!-- <div style="float:left;color:#808080">Archiki Prasad<br style="clear:both" /> </div> -->
<div class="row">
<div class="column"><a style="float:left;color:#808080"><span>Archiki Prasad</span></a>
<br style="clear:both" />
<a href="mailto:archikiprasad@gmail.com" style="float:left;"><span>archikiprasad@gmail.com</span></a>
<br style="clear:both" />
<a href="mailto:archiki@iitb.ac.in" style="float:left;"><span>archiki@iitb.ac.in</span></a>
</div>
<!-- <div class="row"> -->
<div class="column">
<a href="https://github.com/archiki" style="float:left;"><span class="icon icon--github"><svg x="0px" y="0px" width="16px" height="16px" viewBox="0 0 16 16"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">archiki</span></a>
<br style="clear:both" />
<a href="https://www.linkedin.com/in/archiki-prasad" style="float:left;"><span class="icon icon--linkedin"><svg  x="0px" y="0px" width="16px" height="16px" viewBox="0 -20 512 512"><path fill="#828282" d="M150.65,100.682c0,27.992-22.508,50.683-50.273,50.683c-27.765,0-50.273-22.691-50.273-50.683
    C50.104,72.691,72.612,50,100.377,50C128.143,50,150.65,72.691,150.65,100.682z M143.294,187.333H58.277V462h85.017V187.333z
    M279.195,187.333h-81.541V462h81.541c0,0,0-101.877,0-144.181c0-38.624,17.779-61.615,51.807-61.615
    c31.268,0,46.289,22.071,46.289,61.615c0,39.545,0,144.181,0,144.181h84.605c0,0,0-100.344,0-173.915
    s-41.689-109.131-99.934-109.131s-82.768,45.369-82.768,45.369V187.333z"/></svg></span><span class="username">&nbsp;Archiki Prasad</span></a>
<br style="clear:both" />
<a href="https://www.facebook.com/archiki1407" style="float:left;"><span class="icon icon--facebook"><svg x="0px" y="0px" width="16px" height="16px" viewBox="0 0 20 20"><path fill="#828282" d="M11.344,5.71c0-0.73,0.074-1.122,1.199-1.122h1.502V1.871h-2.404c-2.886,0-3.903,1.36-3.903,3.646v1.765h-1.8V10h1.8v8.128h3.601V10h2.403l0.32-2.718h-2.724L11.344,5.71z"></path>
</svg></span><span class="username">&nbsp;archiki1407</span></a>
</div>
</div>

        </footer>
      </div>
    </div>

    

  </body>
</html>

