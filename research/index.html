<!DOCTYPE html>
<html>
  <head>
    <title>Archiki Prasad</title>
    <base target="_blank">
        <meta charset="utf-8" />
    <meta content='text/html; charset=utf-8' http-equiv='Content-Type'>
    <meta http-equiv='X-UA-Compatible' content='IE=edge'>
    <meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>

    
    <!-- <meta name="description" content="Electrical Engineering Dual-Degree Student, IIT Bombay">
    <meta property="og:description" content="Electrical Engineering Dual-Degree Student, IIT Bombay" /> -->
    
    <meta name="author" content="Archiki Prasad" />

    

    <!--[if lt IE 9]>
      <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link href="https://fonts.googleapis.com/css?family=Roboto+Slab:300,400,700" rel="stylesheet">
    <link rel="alternate" type="application/rss+xml" title="Archiki Prasad - Electrical Engineering Dual-Degree Student, IIT Bombay" href="/feed.xml" />
    <link rel="icon" type="image/png" href="/files/iitb.png">
    <!-- Created with Jekyll Now - http://github.com/barryclark/jekyll-now -->
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
        <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>
  </head>

  <body>
    <div class="wrapper-masthead">
      <div class="container">
        <header class="masthead clearfix">
          
          <div class="site-info">
            <h1 class="site-name"><a href="/" target="_self">Archiki Prasad</a></h1>
            <!-- <p class="site-description">Electrical Engineering Dual-Degree Student, IIT Bombay</p> -->
          </div>

          <nav>
            <a href="/about" target="_self">About</a>
            <a href="/research" target="_self">Research</a>
            <a href="/files/Resume.pdf">Resume</a>
            <a href="/files/CV.pdf">CV</a>
          </nav>
        </header>
      </div>
    </div>

<!-- <div id="archiki_acl20_bib" class="entry" style="display:none;">  -->
                           <!-- </br>
<pre>
@inproceedings{prasad2020accents,
  title={How Accents Confound: Probing for Accent Information in End-to-End Speech Recognition Systems},
  author={Prasad, Archiki and Jyothi, Preethi},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={3739--3753},
  year={2020}
}
}
</pre>
</div> -->
    <div id="main" role="main" class="container">
      <article class="post">
  <h1>Research</h1>

  <div class="entry">
    <p>My research interests broadly lie in the fields of Machine Learning, Natural Language Processing and Speech Processing. In the past, I have worked on time-series and sequence-to-sequence tasks. I am interested in explainability and interpretability of black-box models in Natural Language Processing and Automatic Speech Recognition. I am also interested in building machine learning models that robust to distributional shifts and are not dependent on dataset biases or spurious correlations. Recently, I have been fascinated by problems around context in converstational and dialogue systems. </p>

<h3 id="publications">PUBLICATIONS & PATENTS</h3>

<ul>
  <li>
    <p><b><a href="https://www.aclweb.org/anthology/2020.acl-main.345.pdf">How Accents Confound: Probing for Accent Information in End-to-End Speech Recognition Systems</a></b><br />
 <strong>Archiki Prasad</strong> and Preethi Jyothi<br />
 <b><a href="https://www.aclweb.org/anthology/volumes/2020.acl-main/">ACL 2020</a></b><br /> 
<!-- [<a href="#archiki_acl20_bib">bib</a>] -->
[<a href="#" onclick="$('#archiki_acl20_abstract').toggle();return false;" style="font-variant: small-caps;">abstract</a>]                          
[<a href="https://github.com/archiki/ASR-Accent-Analysis" style="font-variant: small-caps;">code</a>] [<a href="https://slideslive.com/38929438/how-accents-confound-probing-for-accent-information-in-endtoend-speech-recognition-systems" style="font-variant: small-caps;">talk</a>]
<div id="archiki_acl20_abstract" class="abstract" style="display:none;">
</br>
    <p>
        In this work, we present a detailed analysis of how accent information is reflected in the internal representation of speech in an end-to-end automatic speech recognition (ASR) system. We use a state-of-the-art end-to-end ASR system, comprising convolutional and recurrent layers, that is trained on a large amount of US-accented English speech and evaluate the model on speech samples from seven different English accents. We examine the effects of accent on the internal representation using three main probing techniques: a) Gradient-based explanation methods, b) Information-theoretic measures, and c) Outputs of accent and phone classifiers. We find different accents exhibiting similar trends irrespective of the probing technique used. We also find that most accent information is encoded within the first recurrent layer, which is suggestive of how one could adapt such an end-to-end model to learn representations that are invariant to accents.
    </p>
</div>
</p>

  </li>
  <li>
    <p><b><a href="https://dl.acm.org/doi/pdf/10.1145/3366424.3382728">Time Series Forecasting for Cold-Start Items by Learning from Related Items using Memory Networks</a></b><br />
Ayush Chauhan, <strong>Archiki Prasad</strong>, Parth Gupta, Amiredddy Prashanth Reddy and Shiv Kumar Saini<br />
<b><a href="https://www2020.thewebconf.org/">The Web Conference (WWW) 2020 </a></b><br /> 
[<a href="#" onclick="$('#archiki_www20_abstract').toggle();return false;" style="font-variant: small-caps;">abstract</a>]
<div id="archiki_www20_abstract" class="abstract" style="display:none;">
</br>
    <p>
        Time series forecasting for new items is very important in a wide variety of applications. Existing solutions for time series forecasting, however, do not address this cold start problem. The underlying machine learning models in these solutions rely heavily on the availability of the past data points of the time series. Here, we propose to use a modified Dynamic Key-Value Memory Network (DKVMN) that enables knowledge sharing across items. The network is conventionally used for binary tasks in knowledge tracing. We modify it for our regression-based forecasting use-case. Specifically, we change the output layer, include feedback for error correction, add a mechanism to handle scale across items. We test our solution on the SKU level data of a large e-commerce company and compare the results to the widely used LSTM model, outperforming it by over 25% across multiple metrics.
    </p>
</div>
</p>
  </li>
  <li>
    <p><b style="color: #800000">Key-Value Memory Networks for Predicting Time Series Metrics of Target Entities</b><br />
Shiv Kumar Saini, Ayush Chauhan, Parth Gupta, <strong>Archiki Prasad</strong>, Ritwick Chaudhary and Amiredddy Prashanth Reddy<br />
<em>Patent filed at the </em><b><a href="https://www.uspto.gov/">US Patent and Trademarks Office</a></b> <em>2020 | Adobe Inc.</em><br />
[<a href="#" onclick="$('#archiki_patent').toggle();return false;" style="font-variant: small-caps;">summary</a>] [<span style="font-variant: small-caps;">Application No. US16/868942</span>]
<div id="archiki_patent" class="abstract" style="display:none;">
</br>
    <p>
        This disclosure involves using key-value memory networks to predict time-series data. For instance, a computing system retrieves, for a target entity, static feature data and target time-series feature data. The computing system can normalize the target time-series feature data based on a normalization scale. The computing system also generates input data by, for example, concatenating the static feature data, the normalized time-series feature data, and time-specific feature data. The computing system generates predicted time-series data for the target metric of the target entity by applying a key-value memory network to the input data. The key-value memory network can include a key matrix learned from training static feature data and training time-series feature data, a value matrix representing time-series trends, and an output layer with a continuous activation function for generating predicted time-series data.
    </p>
</div>
</p>
  </li>
  <li>
  <p><b><a href="https://archiki.github.io/files/wcnc-prasad.pdf">Decentralized Age-of-Information Bandits</a></b><br />
<strong>Archiki Prasad</strong>, Vishal Jain and Sharayu Moharir<br />
<b><a href="https://wcnc2021.ieee-wcnc.org/">IEEE-WCNC 2021</a></b><br /> 
[<a href="#" onclick="$('#archiki_wcnc21_abstract').toggle();return false;" style="font-variant: small-caps;">abstract</a>] [<a href="https://arxiv.org/pdf/2009.12961.pdf" style="font-variant: small-caps;">long-form with proofs</a>]
<div id="archiki_wcnc21_abstract" class="abstract" style="display:none;">
</br>
    <p>
        Age-of-Information (AoI) is a performance metric for scheduling systems that measures the freshness of the data available at the intended destination. AoI is formally defined as the time elapsed since the destination received the recent most update from the source. We consider the problem of scheduling to minimize the cumulative AoI in a multi-source multi-channel setting. Our focus is on the setting where channel statistics are unknown and we model the problem as a distributed multi-armed bandit problem. For an appropriately defined AoI regret metric, we provide analytical performance guarantees of an existing UCB-based policy for the distributed multi-armed bandit problem. In addition, we propose a novel policy based on Thomson Sampling and a hybrid policy that tries to balance the trade-off between the aforementioned policies. Further, we develop AoI-aware variants of these policies in which each source takes its current AoI into account while making decisions. We compare the performance of various policies via simulations. 
    </p>
</div>
</p>
  </li>
  <li>
  <p><b><a href="https://archiki.github.io/files/ICASSP.pdf">An Investigation of End-to-End Models for Robust Speech Recognition</a></b> <em> (Pre-Print) </em><br />
<strong>Archiki Prasad</strong>, Preethi Jyothi and Rajbabu Velmurugan<br />
<em>Submitted to </em><b><a href="https://2021.ieeeicassp.org/">IEEE-ICASSP 2021</a></b><br /> 
[<a href="#" onclick="$('#archiki_icassp21_abstract').toggle();return false;" style="font-variant: small-caps;">abstract</a>] 
<div id="archiki_icassp21_abstract" class="abstract" style="display:none;">
</br>
    <p>
        End-to-end models for robust automatic speech recognition (ASR) have not been sufficiently well-explored in prior work. With end-to-end models, one could choose to preprocess the input speech using speech enhancement techniques and train the model using enhanced speech. Another alternative is to pass the noisy speech as input and modify the model architecture to adapt to noisy speech. A systematic comparison of these two approaches for end-to-end robust ASR has not been attempted before. We address this gap and present a detailed comparison of speech enhancement-based techniques and three different model-based adaptation techniques covering data augmentation, multi-task learning, and adversarial learning for robust ASR. While adversarial learning is the best-performing technique on certain noise types, it comes at a cost of degrading clean speech WER. On other relatively stationary noise types, a new speech enhancement technique outperformed all the model-based daptation techniques. This suggests that knowledge of the underlying noise type can meaningfully inform the choice of adaptation technique.
            </p>
</div>
</p>
  </li>
</ul>

<!-- <p><sup>†</sup><em>Equal Contribution</em></p> -->

<h3 id="research-projects">RESEARCH PROJECTS</h3>
Please have a look at my <a href="/files/CV.pdf">Curriculum Vitae</a> for a comprehensive list of my projects.
<h4 id="2020">2020</h4>
<p><b>Adapting Multilingual BERT for Code-Switching</b> <a href="https://microsoft.github.io/GLUECoS/" button class="button button1">Track GLUECoS NLI Leaderboard!</a><br/>
<em>Guide: <a href="https://www.cse.iitb.ac.in/~pjyothi/">Prof. Preethi Jyothi</a></em></p>

<p><strong style="color:#800000;">> What is code-switching?</strong> Code-Switching occurs when more than one language is used in the same text or conversation. For languages with differenct scripts such as English-Hindi, code-switched data may be in the native script, eg. <strong>Weekend</strong> का क्या <strong>plan</strong> है<strong>?</strong>, or transliterated in the roman script, eg. <strong>Weekend ka kya plan hai?</strong><br/>
<strong style="color:#800000;">> Why is this problem important?</strong> A majority of english speakers around the world are non-native speakers and thus, speak more than one language. For such speakers, there is a tendency to switch between two languages in informal verbal and textual settings such as conversations, social-media posts. However, NLP systems struggle to understand such text. Bridging this gap will enable these systems to have a larger reach.<br/>
<strong style="color:#800000;">> Our focus</strong> is to work on natural language understanding tasks such as natural language inference (NLI) on code-switched data by expanding the capabilities of multilingual BERT which has been trained on large amounts of data in about 104 languages. Specifically, we are exploring an intermediate-task learning frameworks for this goal.  
</p>

<p><b>Joint Noise and Accent Robustness of End-to-End ASR</b><br/>
<em>Guide: <a href="https://www.cse.iitb.ac.in/~pjyothi/">Prof. Preethi Jyothi</a> and <a href="https://www.ee.iitb.ac.in/wiki/faculty/rajbabu">Prof. Rajbabu Velmurugan</a></em></p>

<strong style="color:#800000;">> Why is this problem important?</strong> End-to-end speech recognition datasets are often curated for native speakers in quiet surroundings. In the real-world deployment of these systems, more often than not the speakers have non-native accented speech in the presence of background noise. This out-of-distribution data causes very high degradation in real-world performance.<br/>
<strong style="color:#800000;">> Our focus</strong> is to develop training techniques that can adapt ASR systems trained on clean native speech using low-resource noisy and accented speech. So far, we have been able to show simple machine learning techniques like multi-task learning and adversarial training on end-to-end systems can outperform or match the performance of the state-of-the-art front-end speech enhancement used to tackle noisy speech.
</p>

<p><b>Decentralized Scheduling via Age-of-Information (AoI) Bandits</b><br/>
<em>Guide: <a href="https://www.ee.iitb.ac.in/web/people/faculty/home/sharayum">Prof. Sharayu Moharir</a></em></p>

<p><strong style="color:#800000;">> What is Age-of-Information?</strong> Age-of-Information (AoI) is a performance metric for scheduling systems that measures the freshness of the data available at the intended destination. AoI is formally defined as the time elapsed since the destination received the recent most update from the source. In our case, we minimize the total AoI of a network of sources.<br/>
<strong style="color:#800000;">> Why is this problem important?</strong> With an increasing number of IoT devices, there are several applications in which a large network of (decentralized) devices or sensors need to communicate to a control center using a limited number of channels. In many cases, heavily delayed information is not useful in the decision making of the main control center. The definition of AoI captures this constraint and minimizing this metric is mre challenging than maximizing a bernoulli reward. <br/>
<strong style="color:#800000;">> Our focus</strong> was to use multi-armed bandits to adress this scheduling problem. We have extended scheduling algorithms using Upper Confidence Bound and Thompson Sampling to minimize the AoI and proposed a novel hybrid policy. We have also made metric-aware modifications that further minimize regret. 
</p>

<h4 id="2019">2019</h4>

<p><b>Probing End-to-End ASR for Accent Information</b><br/>
<em>Guide: <a href="https://www.cse.iitb.ac.in/~pjyothi/">Prof. Preethi Jyothi</a></em></p>

<p><strong style="color:#800000;">> What is the effect of accents on ASR?</strong> Most of the standard speech recognition datasets contain speech from only native english speakers often in the American accent. Systems trained on such datasets tend to underperform when subjected to new accent such as UK or Australian English. This performance gap further widens for thick accents such as Indian or Scottish English.<br/>
<strong style="color:#800000;">> Why is this problem important?</strong> Due to the performance disparity and the large number of non-native speakers the problem of accent adaptation has recieved a lot of attention. However, as the field has moved towards end-to-end models that are opaque, there is very limited understanding about how the effect of accents manifests in these networks. Progress in understanding this model behaviour will not only reveal the defects in these systems but can also inform adaptation techniques.<br/>
<strong style="color:#800000;">> Our focus</strong> was to understand how this effect manifests in a renowned end-to-end ASR system called DeepSpeech2. We use a lot of popular interpretability techniques such a saliency maps, information-theoretic measures and probing classifiers that shed light on the model behaviour and how it changes for different accents.

<p><b>Time Series Forecasting of Cold-Start Entities</b><br/>
<em>Guide: <a href="https://research.adobe.com/person/shiv-kumar-saini/">Dr. Shiv Saini</a>, <a href="https://research.adobe.com/">Adobe Research</a> (Internship)</em></p>

<p><strong style="color:#800000;">> What are cold-start entities/products?</strong> These are products that have little or no historical data. Often, newly launched products fit into this category.</strong><br/>
<strong style="color:#800000;">> Why is this problem important?</strong> Most machine learning-based solutions for time-series forecasting rely on historical sequential data and fail to generate any reasonable forecasts for cold-start products. At the same time, these forecasts for these products are extremely valuable to end-users like analysts to make crucial decisions early into the product life-cycle and manage inventory.<br/>
<strong style="color:#800000;">> Our focus</strong> was to design a framework that maximizes learning between cold-start and data-rich products and use all the textual meta-information available about the product. For this we used a Key-Value Memory network which reinforces cross-learning and outperforms the LSTM baseline. An additional goal of this project was to track the products that were similar as per the model at different time steps.



  </div>

  <!--div class="date">
    Written on 
  </div
//-->

  
</article>
</div>

    <div class="wrapper-footer">
      <div class="container">
        <footer class="footer">
<!-- <div style="float:left;color:#808080">Archiki Prasad<br style="clear:both" /> </div> -->
<div class="row">
<div class="column"><a style="float:left;color:#808080"><span>Archiki Prasad</span></a>
<br style="clear:both" />
<a href="mailto:archikiprasad@gmail.com" style="float:left;font-weight:lighter;"><span>archikiprasad@gmail.com</span></a>
<br style="clear:both" />
<a href="mailto:archiki@iitb.ac.in" style="float:left;font-weight:lighter;"><span>archiki@iitb.ac.in</span></a>
</div>
<!-- <div class="row"> -->
<div class="column">
<a href="https://github.com/archiki" style="float:left;font-weight:lighter;"><span class="icon icon--github"><svg x="0px" y="0px" width="16px" height="16px" viewBox="0 0 16 16"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">archiki</span></a>
<br style="clear:both" />
<a href="https://www.linkedin.com/in/archiki-prasad" style="float:left;font-weight:lighter;"><span class="icon icon--linkedin"><svg  x="0px" y="0px" width="16px" height="16px" viewBox="0 -20 512 512"><path fill="#828282" d="M150.65,100.682c0,27.992-22.508,50.683-50.273,50.683c-27.765,0-50.273-22.691-50.273-50.683
    C50.104,72.691,72.612,50,100.377,50C128.143,50,150.65,72.691,150.65,100.682z M143.294,187.333H58.277V462h85.017V187.333z
    M279.195,187.333h-81.541V462h81.541c0,0,0-101.877,0-144.181c0-38.624,17.779-61.615,51.807-61.615
    c31.268,0,46.289,22.071,46.289,61.615c0,39.545,0,144.181,0,144.181h84.605c0,0,0-100.344,0-173.915
    s-41.689-109.131-99.934-109.131s-82.768,45.369-82.768,45.369V187.333z"/></svg></span><span class="username">&nbsp;Archiki Prasad</span></a>
<br style="clear:both" />
<a href="https://www.facebook.com/archiki1407" style="float:left;font-weight:lighter;"><span class="icon icon--facebook"><svg x="0px" y="0px" width="16px" height="16px" viewBox="0 0 20 20"><path fill="#828282" d="M11.344,5.71c0-0.73,0.074-1.122,1.199-1.122h1.502V1.871h-2.404c-2.886,0-3.903,1.36-3.903,3.646v1.765h-1.8V10h1.8v8.128h3.601V10h2.403l0.32-2.718h-2.724L11.344,5.71z"></path>
</svg></span><span class="username">&nbsp;archiki1407</span></a>
</div>
</div>

        </footer>
      </div>
    </div>

    

  </body>
</html>

