<!DOCTYPE html>
<html>
  <head>
    <title>Archiki Prasad</title>
    <base target="_blank">
        <meta charset="utf-8" />
    <meta content='text/html; charset=utf-8' http-equiv='Content-Type'>
    <meta http-equiv='X-UA-Compatible' content='IE=edge'>
    <meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>

    
    <!-- <meta name="description" content="Electrical Engineering Dual-Degree Student, IIT Bombay">
    <meta property="og:description" content="Electrical Engineering Dual-Degree Student, IIT Bombay" /> -->
    
    <meta name="author" content="Archiki Prasad" />

    

    <!--[if lt IE 9]>
      <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <link rel="stylesheet" type="text/css" href="/style.css" />
<!--     <link href="https://fonts.googleapis.com/css?family=Roboto+Slab:300,400,700" rel="stylesheet"> -->
    <link href="https://fonts.googleapis.com/css?family=Ubuntu:300,400,300italic,400italic|Raleway:500,100,300" rel="stylesheet">
    <link rel="alternate" type="application/rss+xml" title="Archiki Prasad - PhD Student in Computer Science, UNC Chapel Hill" href="/feed.xml" />
    <link rel="icon" type="image/png" href="/files/UNC.png">
    <!-- Created with Jekyll Now - http://github.com/barryclark/jekyll-now -->
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
        <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>
  </head>

  <body>
    <div class="wrapper-masthead">
      <div class="container">
        <header class="masthead clearfix">
          
          <div class="site-info">
            <h1 class="site-name"><a href="/" target="_self">Archiki Prasad</a></h1>
            <!-- <p class="site-description">Electrical Engineering Dual-Degree Student, IIT Bombay</p> -->
          </div>

          <nav>
            <a href="/about" target="_self">About</a>
            <a href="/research" target="_self">Research</a>
            <a href="/files/CV.pdf">CV</a>
          </nav>
        </header>
      </div>
    </div>

<!-- <div id="archiki_acl20_bib" class="entry" style="display:none;">  -->
                           <!-- </br>
<pre>
@inproceedings{prasad2020accents,
  title={How Accents Confound: Probing for Accent Information in End-to-End Speech Recognition Systems},
  author={Prasad, Archiki and Jyothi, Preethi},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={3739--3753},
  year={2020}
}
}
</pre>
</div> -->
    <div id="main" role="main" class="container">
      <article class="post">
  <h1>Research</h1>

  <div class="entry">
    <p>My research interests broadly lie in the fields of Machine Learning, Natural Language Processing and Speech Processing. In the past, I have worked on time-series and sequence-to-sequence tasks. Recently, I have been working on reasearch promblems in prompt-based learning and in-context learning with pretrained large language models in zero-shot and few-shot settings as well as evaluation model-generated step-by-step reasoning. I am excited about problems involving (task) instruction-based learning, evaluation of consistency and reasoning abilities of large language models. I am also interested in building machine learning models that robust to distributional shifts and are not dependent on dataset biases or spurious correlations. </p>

<h3 id="publications">Publications and Patents [<a href="https://scholar.google.com/citations?user=Svcwv-IAAAAJ&hl=en">Google Scholar</a>] [<a href="https://www.semanticscholar.org/author/Archiki-Prasad/1677896557">Semantic Scholar</a>]</h3> 

<ul>
  <li>
    <p><b><a href=https://arxiv.org/abs/2401.16467">ReGAL: Refactoring Programs to Discover Generalizable Abstractions</a></b><br/>
Elias Stengel-Eskin, <strong>Archiki Prasad</strong>, Mohit Bansal<br />
<b>Arxiv Preprint</b><br /> 
[<a href="#" onclick="$('#archiki_regal24_abstract').toggle();return false;" style="font-variant: small-caps;">abstract</a>] [<a href="https://github.com/esteng/regal_program_learning" style="font-variant: small-caps;">code</a>] 
<div id="archiki_regal24_abstract" class="abstract" style="display:none;">
</br>
    <p>
While large language models (LLMs) are increasingly being used for program synthesis, they lack the global view needed to develop useful abstractions; they generally predict programs one at a time, often repeating the same functionality. Generating redundant code from scratch is both inefficient and error-prone. To address this, we propose Refactoring for Generalizable Abstraction Learning (ReGAL), a gradient-free method for learning a library of reusable functions via code refactorization, i.e. restructuring code without changing its execution output. ReGAL learns from a small set of existing programs, iteratively verifying and refining its abstractions via execution. We find that the shared function libraries discovered by ReGAL make programs easier to predict across diverse domains. On three datasets (LOGO graphics generation, Date reasoning, and TextCraft, a Minecraft-based text game), both open-source and proprietary LLMs improve in accuracy when predicting programs with ReGAL functions. For CodeLlama-13B, ReGAL results in absolute accuracy increases of 11.5% on graphics, 26.1% on date understanding, and 8.1% on TextCraft, outperforming GPT-3.5 in two of three domains. Our analysis reveals ReGAL's abstractions encapsulate frequently-used subroutines as well as environment dynamics.</p>
</div>
</p>
  </li>
  <li>
    <p><b><a href="https://arxiv.org/abs/2311.05772">ADaPT: As-Needed Decomposition and Planning with Language Models</a></b><br/>
<strong>Archiki Prasad</strong>, Alexander Koller, Mareike Hartmann, Peter Clark, Ashish Sabharwal, Mohit Bansal, and Tushar Khot<br />
<b>Arxiv Preprint</b><br /> 
[<a href="#" onclick="$('#archiki_adapt23_abstract').toggle();return false;" style="font-variant: small-caps;">abstract</a>] [<a href="https://github.com/archiki/ADaPT" style="font-variant: small-caps;">code</a>] [<a href="https://allenai.github.io/adaptllm/" style="font-variant: small-caps;">project page</a>]
<div id="archiki_adapt23_abstract" class="abstract" style="display:none;">
</br>
    <p>
Large Language Models (LLMs) are increasingly being used for interactive decision-making tasks requiring planning and adapting to the environment. Recent works employ LLMs-as-agents in broadly two ways: iteratively determining the next action (iterative executors) or generating plans and executing sub-tasks using LLMs (plan-and-execute). However, these methods struggle with task complexity, as the inability to execute any sub-task may lead to task failure. To address these shortcomings, we introduce As-Needed Decomposition and Planning for complex Tasks (ADaPT), an approach that explicitly plans and decomposes complex sub-tasks as-needed, i.e., when the LLM is unable to execute them. ADaPT recursively decomposes sub-tasks to adapt to both task complexity and LLM capability. Our results demonstrate that ADaPT substantially outperforms established strong baselines, achieving success rates up to 28.3% higher in ALFWorld, 27% in WebShop, and 33% in TextCraft -- a novel compositional dataset that we introduce. Through extensive analysis, we illustrate the importance of multilevel decomposition and establish that ADaPT dynamically adjusts to the capabilities of the executor LLM as well as to task complexity.            </p>
</p>
</div>
</p>
  </li>
  <li>
    <p><b><a href="https://arxiv.org/abs/2310.05861">Rephrase, Augment, Reason: Visual Grounding of Questions for Vision-Language Models</a></b><br/>
<strong>Archiki Prasad</strong>, Elias Stengel-Eskin and Mohit Bansal<br />
<b><a href="https://iclr.cc/Conferences/2024/">ICLR 2024</a></b><br /> 
[<a href="#" onclick="$('#archiki_repare23_abstract').toggle();return false;" style="font-variant: small-caps;">abstract</a>] [<a href="https://github.com/archiki/RepARe" style="font-variant: small-caps;">code</a>]
<div id="archiki_repare23_abstract" class="abstract" style="display:none;">
</br>
    <p>
      An increasing number of vision-language tasks can be handled with little to no training, i.e., in a zero and few-shot manner, by marrying large language models (LLMs) to vision encoders, resulting in large vision-language models (LVLMs). While this has huge upsides, such as not requiring training data or custom architectures, how an input is presented to a LVLM can have a major impact on zero-shot model performance. In particular, inputs phrased in an underspecified way can result in incorrect answers due to factors like missing visual information, complex implicit reasoning, or linguistic ambiguity. Therefore, adding visually grounded information to the input as a preemptive clarification should improve model performance by reducing underspecification, e.g., by localizing objects and disambiguating references. Similarly, in the VQA setting, changing the way questions are framed can make them easier for models to answer. To this end, we present Rephrase, Augment and Reason (RepARe), a gradient-free framework that extracts salient details about the image using the underlying LVLM as a captioner and reasoner, in order to propose modifications to the original question. We then use the LVLM's confidence over a generated answer as an unsupervised scoring function to select the rephrased question most likely to improve zero-shot performance. Focusing on three visual question answering tasks, we show that RepARe can result in a 3.85% (absolute) increase in zero-shot accuracy on VQAv2, 6.41%, and 7.94% points increase on A-OKVQA, and VizWiz respectively. Additionally, we find that using gold answers for oracle question candidate selection achieves a substantial gain in VQA accuracy by up to 14.41%. Through extensive analysis, we demonstrate that outputs from RepARe increase syntactic complexity, and effectively utilize vision-language interaction and the frozen language model in LVLMs.
    </p>
</div>
</p>
  </li>
  <li>
    <p><b><a href="https://arxiv.org/abs/2304.10703">ReCEval: Evaluating Reasoning Chains via Correctness and Informativeness</a></b><br/>
<strong>Archiki Prasad</strong>, Swarnadeep Saha, Xiang Zhou and Mohit Bansal<br />
<b><a href="https://2023.emnlp.org/">EMNLP 2023</a></b><br /> 
[<a href="#" onclick="$('#archiki_receval23_abstract').toggle();return false;" style="font-variant: small-caps;">abstract</a>] [<a href="https://github.com/archiki/ReCEval" style="font-variant: small-caps;">code</a>]
<div id="archiki_receval23_abstract" class="abstract" style="display:none;">
</br>
    <p>
      Multi-step reasoning ability is fundamental to many natural language tasks, yet it is unclear what constitutes a good reasoning chain and how to evaluate them. Most existing methods focus solely on whether the reasoning chain leads to the correct conclusion, but this answer-oriented view may confound the quality of reasoning with other spurious shortcuts to predict the answer. To bridge this gap, we evaluate reasoning chains by viewing them as informal proofs that derive the final answer. Specifically, we propose ReCEval (Reasoning Chain Evaluation), a framework that evaluates reasoning chains through two key properties: (1) correctness, i.e., each step makes a valid inference based on the information contained within the step, preceding steps, and input context, and (2) informativeness, i.e., each step provides new information that is helpful towards deriving the generated answer. We implement ReCEval using natural language inference models and information-theoretic measures. On multiple datasets, ReCEval is highly effective in identifying different types of errors, resulting in notable improvements compared to prior methods. We demonstrate that our informativeness metric captures the expected flow of information in high-quality reasoning chains and we also analyze the impact of previous steps on evaluating correctness and informativeness. Finally, we show that scoring reasoning chains based on ReCEval can improve downstream performance of reasoning tasks.
            </p>
</div>
</p>
  </li>
  <li>
    <p><b><a href="https://aclanthology.org/2023.acl-long.837/">MeetingQA: Extractive Question-Answering on Meeting Transcripts</a></b><br/>
<strong>Archiki Prasad</strong>, Trung Bui, Seunghyun Yoon, Hanieh Deilamsalehy, Franck Dernoncourt and Mohit Bansal<br />
<b><a href="https://2023.aclweb.org/">ACL 2023</a></b><br /> 
[<a href="#" onclick="$('#archiki_meetQA23_abstract').toggle();return false;" style="font-variant: small-caps;">abstract</a>] [<a href="https://github.com/adobe-research/meetingqa/" style="font-variant: small-caps;">code + data</a>] [<a href="https://archiki.github.io/meetingqa.html" style="font-variant: small-caps;">project page</a>]
<div id="archiki_meetQA23_abstract" class="abstract" style="display:none;">
</br>
    <p>
      With the ubiquitous use of online meeting platforms and robust automatic speech recognition systems, meeting transcripts have emerged as a new and interesting domain for natural language tasks. Most recent works on meeting transcripts are restricted to summarization and extraction of action items. However, meeting discussions also have a useful question-answering (QA) component, crucial to understanding the discourse or meeting content, and can be used to build interactive interfaces on top of long transcripts. Hence, in this work, we leverage this inherent QA component of meeting discussions and introduce MeetingQA, an extractive QA dataset comprising questions asked by meeting participants and corresponding responses. As a result, questions can be open-ended and seek active discussions, while the answers can be multi-span and spread across multiple speakers. Our comprehensive empirical study of several robust baselines including long-context language models and recent instruction-tuned models reveals that models perform poorly on this task (F1 = 57.3) and severely lag behind human performance (F1 = 84.6), thus presenting a useful, challenging new task for the community to improve upon.
            </p>
</div>
</p>
  </li>
  <li>
    <p><b><a href="https://archiki.github.io/files/EACLCameraReady.pdf">GrIPS: Gradient-free, Edit-based Instruction Search for Prompting Large Language Models</a></b> <br />
<strong>Archiki Prasad</strong>, Peter Hase, Xiang Zhou and Mohit Bansal<br />
<b><a href="https://2023.eacl.org/">EACL 2023</a></b><br /> 
[<a href="#" onclick="$('#archiki_grips22_abstract').toggle();return false;" style="font-variant: small-caps;">abstract</a>] [<a href="https://github.com/archiki/GrIPS" style="font-variant: small-caps;">code</a>]
<div id="archiki_grips22_abstract" class="abstract" style="display:none;">
</br>
    <p>
      Providing natural language instructions in prompts is a useful new paradigm for improving task performance of large language models in a zero-shot setting. Recent work has aimed to improve such prompts via manual rewriting or gradient-based tuning. However, manual rewriting is time-consuming and requires subjective interpretation, while gradient-based tuning can be extremely computationally demanding for large models and requires full access to model weights, which may not be available for API-based models. In this work, we introduce Gradient-free Instructional Prompt Search (GrIPS), a gradient-free, edit-based search approach for improving task instructions for large language models. GrIPS takes in instructions designed for humans and automatically returns an improved, edited prompt, while allowing for API-based tuning. The instructions in our search are iteratively edited using four operations (delete, add, swap, paraphrase) on text at the phrase-level. With InstructGPT models, GrIPS improves the average task performance by up to 4.30 percentage points on eight classification tasks from the Natural-Instructions dataset (with similar improvements
for OPT, BLOOM, and FLANT5). We see improvements for both instruction-only prompts and for k-shot example+instruction prompts. Notably, GrIPS outperforms manual rewriting following the guidelines in Mishra et al. (2022) and also outperforms purely example-based prompts while controlling for the available compute and data budget. Further, performance of GRIPS is
comparable to select gradient-based tuning approaches. Qualitatively, we show our edits can simplify instructions and at times make them incoherent but nonetheless improve accuracy.
            </p>
</div>
</p>
  </li>

  <li>
    <p><b><a href="https://aclanthology.org/2021.mrl-1.16.pdf">The Effectiveness of Intermediate-Task Training for Code-Switched Natural Language Understanding</a></b> <br />
<strong>Archiki Prasad*</strong>, Mohammad Ali Rehan*, Shreya Pathak* and Preethi Jyothi<br />
<b><a href="https://sites.google.com/view/mrl-2021/home">Workshop on Multilingual Representation Learning (MRL) 2021</a>, <a href="https://2021.emnlp.org/">EMNLP 2021</a> </b><br /> 
<strong>üèÜ Best Paper Honorable Mention</strong><br />
[<a href="#" onclick="$('#archiki_cos21_abstract').toggle();return false;" style="font-variant: small-caps;">abstract</a>] [<a href="https://github.com/archiki/Intermediate-Task-Code-Switching" style="font-variant: small-caps;">code</a>] [<a href="https://archiki.github.io/files/EMNLP_MRL.pptx" style="font-variant: small-caps">slides</a>]
<div id="archiki_cos21_abstract" class="abstract" style="display:none;">
</br>
    <p>
      While recent benchmarks have spurred a lot of new work on improving the generalization of pretrained multilingual language models on multilingual tasks, techniques to improve code-switched natural language understanding tasks have been far less explored. In this work, we propose the use of bilingual intermediate pretraining as a reliable technique to derive large and consistent performance gains on three different NLP tasks using code-switched text. We achieve substantial absolute improvements of 7.87%, 20.15%, and 10.99%, on the mean accuracies and F1 scores over previous state-of-the-art systems for Hindi-English Natural Language Inference (NLI), Question Answering (QA) tasks, and Spanish-English Sentiment Analysis (SA) respectively. We show consistent performance gains on four different code-switched language-pairs (Hindi-English, Spanish-English, Tamil-English and Malayalam-English) for SA. We also present a code-switched masked language modelling (MLM) pretraining technique that consistently benefits SA compared to standard MLM pretraining using real code-switched text.
            </p>
</div>
</p>
  </li>

  <li>
    <p><b><a href="https://arxiv.org/pdf/2102.06237.pdf">An Investigation of End-to-End Models for Robust Speech Recognition</a></b> <br />
  <strong>Archiki Prasad</strong>, Preethi Jyothi and Rajbabu Velmurugan<br />
  <b><a href="https://2021.ieeeicassp.org/">IEEE-ICASSP 2021</a></b><br /> 
  [<a href="#" onclick="$('#archiki_icassp21_abstract').toggle();return false;" style="font-variant: small-caps;">abstract</a>] [<a href="https://github.com/archiki/Robust-E2E-ASR" style="font-variant: small-caps;">code</a>] [<a href="https://archiki.github.io/files/ICASSPposter.pdf" style="font-variant: small-caps">poster</a>]
  <div id="archiki_icassp21_abstract" class="abstract" style="display:none;">
  </br>
      <p>
          End-to-end models for robust automatic speech recognition (ASR) have not been sufficiently well-explored in prior work. With end-to-end models, one could choose to preprocess the input speech using speech enhancement techniques and train the model using enhanced speech. Another alternative is to pass the noisy speech as input and modify the model architecture to adapt to noisy speech. A systematic comparison of these two approaches for end-to-end robust ASR has not been attempted before. We address this gap and present a detailed comparison of speech enhancement-based techniques and three different model-based adaptation techniques covering data augmentation, multi-task learning, and adversarial learning for robust ASR. While adversarial learning is the best-performing technique on certain noise types, it comes at a cost of degrading clean speech WER. On other relatively stationary noise types, a new speech enhancement technique outperformed all the model-based daptation techniques. This suggests that knowledge of the underlying noise type can meaningfully inform the choice of adaptation technique.
              </p>
  </div>
  </p>
    </li>

    <li>
      <p><b><a href="https://archiki.github.io/files/wcnc-prasad.pdf">Decentralized Age-of-Information Bandits</a></b><br />
    <strong>Archiki Prasad</strong>, Vishal Jain and Sharayu Moharir<br />
    <b><a href="https://wcnc2021.ieee-wcnc.org/">IEEE-WCNC 2021</a></b><br /> 
    [<a href="#" onclick="$('#archiki_wcnc21_abstract').toggle();return false;" style="font-variant: small-caps;">abstract</a>] [<a href="https://arxiv.org/pdf/2009.12961.pdf" style="font-variant: small-caps;">long-form with proofs</a>]
    <div id="archiki_wcnc21_abstract" class="abstract" style="display:none;">
    </br>
        <p>
            Age-of-Information (AoI) is a performance metric for scheduling systems that measures the freshness of the data available at the intended destination. AoI is formally defined as the time elapsed since the destination received the recent most update from the source. We consider the problem of scheduling to minimize the cumulative AoI in a multi-source multi-channel setting. Our focus is on the setting where channel statistics are unknown and we model the problem as a distributed multi-armed bandit problem. For an appropriately defined AoI regret metric, we provide analytical performance guarantees of an existing UCB-based policy for the distributed multi-armed bandit problem. In addition, we propose a novel policy based on Thomson Sampling and a hybrid policy that tries to balance the trade-off between the aforementioned policies. Further, we develop AoI-aware variants of these policies in which each source takes its current AoI into account while making decisions. We compare the performance of various policies via simulations. 
        </p>
    </div>
    </p>
      </li>
      
  <li>
    <p><b><a href="https://www.aclweb.org/anthology/2020.acl-main.345.pdf">How Accents Confound: Probing for Accent Information in End-to-End Speech Recognition Systems</a></b><br />
 <strong>Archiki Prasad</strong> and Preethi Jyothi<br />
 <b><a href="https://www.aclweb.org/anthology/volumes/2020.acl-main/">ACL 2020</a></b><br /> 
<!-- [<a href="#archiki_acl20_bib">bib</a>] -->
[<a href="#" onclick="$('#archiki_acl20_abstract').toggle();return false;" style="font-variant: small-caps;">abstract</a>]                          
[<a href="https://github.com/archiki/ASR-Accent-Analysis" style="font-variant: small-caps;">code</a>] [<a href="https://slideslive.com/38929438/how-accents-confound-probing-for-accent-information-in-endtoend-speech-recognition-systems" style="font-variant: small-caps;">talk</a>]
<div id="archiki_acl20_abstract" class="abstract" style="display:none;">
</br>
    <p>
        In this work, we present a detailed analysis of how accent information is reflected in the internal representation of speech in an end-to-end automatic speech recognition (ASR) system. We use a state-of-the-art end-to-end ASR system, comprising convolutional and recurrent layers, that is trained on a large amount of US-accented English speech and evaluate the model on speech samples from seven different English accents. We examine the effects of accent on the internal representation using three main probing techniques: a) Gradient-based explanation methods, b) Information-theoretic measures, and c) Outputs of accent and phone classifiers. We find different accents exhibiting similar trends irrespective of the probing technique used. We also find that most accent information is encoded within the first recurrent layer, which is suggestive of how one could adapt such an end-to-end model to learn representations that are invariant to accents.
    </p>
</div>
</p>

  </li>
  <li>
    <p><b><a href="https://dl.acm.org/doi/pdf/10.1145/3366424.3382728">Time Series Forecasting for Cold-Start Items by Learning from Related Items using Memory Networks</a></b><br />
Ayush Chauhan, <strong>Archiki Prasad</strong>, Parth Gupta, Amiredddy Prashanth Reddy and Shiv Kumar Saini<br />
<b><a href="https://www2020.thewebconf.org/">The Web Conference (WWW) 2020 </a></b><br /> 
[<a href="#" onclick="$('#archiki_www20_abstract').toggle();return false;" style="font-variant: small-caps;">abstract</a>]
<div id="archiki_www20_abstract" class="abstract" style="display:none;">
</br>
    <p>
        Time series forecasting for new items is very important in a wide variety of applications. Existing solutions for time series forecasting, however, do not address this cold start problem. The underlying machine learning models in these solutions rely heavily on the availability of the past data points of the time series. Here, we propose to use a modified Dynamic Key-Value Memory Network (DKVMN) that enables knowledge sharing across items. The network is conventionally used for binary tasks in knowledge tracing. We modify it for our regression-based forecasting use-case. Specifically, we change the output layer, include feedback for error correction, add a mechanism to handle scale across items. We test our solution on the SKU level data of a large e-commerce company and compare the results to the widely used LSTM model, outperforming it by over 25% across multiple metrics.
    </p>
</div>
</p>
  </li>
  <li>
    <p><b><a href="https://uspto.report/patent/app/20210350175">Key-Value Memory Networks for Predicting Time Series Metrics of Target Entities</a></b><br />
Shiv Kumar Saini, Ayush Chauhan, Parth Gupta, <strong>Archiki Prasad</strong>, Amiredddy Prashanth Reddy and Ritwick Chaudhary <br />
<em>Patent filed at the </em><b><a href="https://www.uspto.gov/">US Patent and Trademarks Office</a></b> <em>2020 | Adobe Inc.</em><br />
[<a href="#" onclick="$('#archiki_patent').toggle();return false;" style="font-variant: small-caps;">summary</a>] [<a href="https://patents.google.com/patent/US20210350175A1/en" style="font-variant: small-caps;">application no.<small> US16/868942</small></a>]
<div id="archiki_patent" class="abstract" style="display:none;">
</br>
    <p>
        This disclosure involves using key-value memory networks to predict time-series data. For instance, a computing system retrieves, for a target entity, static feature data and target time-series feature data. The computing system can normalize the target time-series feature data based on a normalization scale. The computing system also generates input data by, for example, concatenating the static feature data, the normalized time-series feature data, and time-specific feature data. The computing system generates predicted time-series data for the target metric of the target entity by applying a key-value memory network to the input data. The key-value memory network can include a key matrix learned from training static feature data and training time-series feature data, a value matrix representing time-series trends, and an output layer with a continuous activation function for generating predicted time-series data.
    </p>
</div>
</p>
  </li>
  
  

</ul>

<!-- <p><sup>‚Ä†</sup><em>Equal Contribution</em></p> -->

<h3 id="research-projects">Research Projects</h3>
Please have a look at my <a href="/files/CV.pdf">Curriculum Vitae</a> for a comprehensive list of my projects.
<!--
<h4 id="2020">2020</h4>
<p><b>Adapting Multilingual BERT for Code-Switching</b> <br/>
<em>Guide: <a href="https://www.cse.iitb.ac.in/~pjyothi/">Prof. Preethi Jyothi</a>, <a href="http://www.iitb.ac.in/">IIT Bombay</a></em></p>

<p><strong style="color:#800000;">> What is code-switching?</strong> Code-Switching occurs when more than one language is used in the same text or conversation. For languages with differenct scripts such as English-Hindi, code-switched data may be in the native script, eg. <strong>Weekend</strong> ‡§ï‡§æ ‡§ï‡•ç‡§Ø‡§æ <strong>plan</strong> ‡§π‡•à<strong>?</strong>, or transliterated in the roman script, eg. <strong>Weekend ka kya plan hai?</strong><br/>
<strong style="color:#800000;">> Why is this problem important?</strong> A majority of english speakers around the world are non-native speakers and thus, speak more than one language. For such speakers, there is a tendency to switch between two languages in informal verbal and textual settings such as conversations, social-media posts. However, NLP systems struggle to understand such text. Bridging this gap will enable these systems to have a larger reach.<br/>
<strong style="color:#800000;">Note: I have removed the project description in honor of the anonymity period of an ongoing submission. Please reach out to me in case of any queries.</strong>

</p>

<p><b>Joint Noise and Accent Robustness of End-to-End ASR</b><br/>
<em>Guide: <a href="https://www.cse.iitb.ac.in/~pjyothi/">Prof. Preethi Jyothi</a> and <a href="https://www.ee.iitb.ac.in/wiki/faculty/rajbabu">Prof. Rajbabu Velmurugan</a>, <a href="http://www.iitb.ac.in/">IIT Bombay</a></em></p>

<strong style="color:#800000;">> Why is this problem important?</strong> End-to-end speech recognition datasets are often curated for native speakers in quiet surroundings. In the real-world deployment of these systems, more often than not the speakers have non-native accented speech in the presence of background noise. This out-of-distribution data causes very high degradation in real-world performance.<br/>
<strong style="color:#800000;">> Our focus</strong> is to develop training techniques that can adapt ASR systems trained on clean native speech using low-resource noisy and accented speech. So far, we have been able to show simple machine learning techniques like multi-task learning and adversarial training on end-to-end systems can outperform or match the performance of the state-of-the-art front-end speech enhancement used to tackle noisy speech.
</p>

<p><b>Decentralized Scheduling via Age-of-Information (AoI) Bandits</b><br/>
<em>Guide: <a href="https://www.ee.iitb.ac.in/web/people/faculty/home/sharayum">Prof. Sharayu Moharir</a>,  <a href="http://www.iitb.ac.in/">IIT Bombay</a></em></p>

<p><strong style="color:#800000;">> What is Age-of-Information?</strong> Age-of-Information (AoI) is a performance metric for scheduling systems that measures the freshness of the data available at the intended destination. AoI is formally defined as the time elapsed since the destination received the recent most update from the source. In our case, we minimize the total AoI of a network of sources.<br/>
<strong style="color:#800000;">> Why is this problem important?</strong> With an increasing number of IoT devices, there are several applications in which a large network of (decentralized) devices or sensors need to communicate to a control center using a limited number of channels. In many cases, heavily delayed information is not useful in the decision making of the main control center. The definition of AoI captures this constraint and minimizing this metric is mre challenging than maximizing a bernoulli reward. <br/>
<strong style="color:#800000;">> Our focus</strong> was to use multi-armed bandits to adress this scheduling problem. We have extended scheduling algorithms using Upper Confidence Bound and Thompson Sampling to minimize the AoI and proposed a novel hybrid policy. We have also made metric-aware modifications that further minimize regret. 
</p>

<h4 id="2019">2019</h4>

<p><b>Probing End-to-End ASR for Accent Information</b><br/>
<em>Guide: <a href="https://www.cse.iitb.ac.in/~pjyothi/">Prof. Preethi Jyothi</a>,  <a href="http://www.iitb.ac.in/">IIT Bombay</a></em></p>

<p><strong style="color:#800000;">> What is the effect of accents on ASR?</strong> Most of the standard speech recognition datasets contain speech from only native english speakers often in the American accent. Systems trained on such datasets tend to underperform when subjected to new accent such as UK or Australian English. This performance gap further widens for thick accents such as Indian or Scottish English.<br/>
<strong style="color:#800000;">> Why is this problem important?</strong> Due to the performance disparity and the large number of non-native speakers the problem of accent adaptation has recieved a lot of attention. However, as the field has moved towards end-to-end models that are opaque, there is very limited understanding about how the effect of accents manifests in these networks. Progress in understanding this model behaviour will not only reveal the defects in these systems but can also inform adaptation techniques.<br/>
<strong style="color:#800000;">> Our focus</strong> was to understand how this effect manifests in a renowned end-to-end ASR system called DeepSpeech2. We use a lot of popular interpretability techniques such a saliency maps, information-theoretic measures and probing classifiers that shed light on the model behaviour and how it changes for different accents.

<p><b>Time Series Forecasting of Cold-Start Entities</b><br/>
<em>Guide: <a href="https://research.adobe.com/person/shiv-kumar-saini/">Dr. Shiv Saini</a>, <a href="https://research.adobe.com/">Adobe Research</a> (Internship)</em></p>

<p><strong style="color:#800000;">> What are cold-start entities/products?</strong> These are products that have little or no historical data. Often, newly launched products fit into this category.</strong><br/>
<strong style="color:#800000;">> Why is this problem important?</strong> Most machine learning-based solutions for time-series forecasting rely on historical sequential data and fail to generate any reasonable forecasts for cold-start products. At the same time, these forecasts for these products are extremely valuable to end-users like analysts to make crucial decisions early into the product life-cycle and manage inventory.<br/>
<strong style="color:#800000;">> Our focus</strong> was to design a framework that maximizes learning between cold-start and data-rich products and use all the textual meta-information available about the product. For this we used a Key-Value Memory network which reinforces cross-learning and outperforms the LSTM baseline. An additional goal of this project was to track the products that were similar as per the model at different time steps.
-->


  </div>

  <!--div class="date">
    Written on 
  </div
//-->

  
</article>
</div>

    <div class="wrapper-footer">
      <div class="container">
        <footer class="footer">
<!-- <div style="float:left;color:#808080">Archiki Prasad<br style="clear:both" /> </div> -->
<div class="row">
<div class="column"><a style="float:left;color:#808080"><span>Archiki Prasad</span></a>
<br style="clear:both" />
<a href="mailto:archikiprasad@gmail.com" style="float:left;font-weight:lighter;"><span>archikiprasad@gmail.com</span></a>
<br style="clear:both" />
<a href="mailto:archiki@cs.unc.edu" style="float:left;font-weight:lighter;"><span>archiki@cs.unc.edu</span></a>
</div>
<!-- <div class="row"> -->
<div class="column">
<a href="https://github.com/archiki" style="float:left;font-weight:lighter;"><span class="icon icon--github"><svg x="0px" y="0px" width="16px" height="16px" viewBox="0 0 16 16"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">archiki</span></a>
<br style="clear:both" />
<a href="https://www.linkedin.com/in/archiki-prasad" style="float:left;font-weight:lighter;"><span class="icon icon--linkedin"><svg  x="0px" y="0px" width="16px" height="16px" viewBox="0 -20 512 512"><path fill="#828282" d="M150.65,100.682c0,27.992-22.508,50.683-50.273,50.683c-27.765,0-50.273-22.691-50.273-50.683
    C50.104,72.691,72.612,50,100.377,50C128.143,50,150.65,72.691,150.65,100.682z M143.294,187.333H58.277V462h85.017V187.333z
    M279.195,187.333h-81.541V462h81.541c0,0,0-101.877,0-144.181c0-38.624,17.779-61.615,51.807-61.615
    c31.268,0,46.289,22.071,46.289,61.615c0,39.545,0,144.181,0,144.181h84.605c0,0,0-100.344,0-173.915
    s-41.689-109.131-99.934-109.131s-82.768,45.369-82.768,45.369V187.333z"/></svg></span><span class="username">&nbsp;Archiki Prasad</span></a>
<br style="clear:both" />
<a href="https://www.twitter.com/archikiprasad" style="float:left;font-weight:lighter;"><span class="icon icon--twitter"><svg x="0px" y="0px" width="16px" height="16px" viewBox="0 -6 20 20"><path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"></path>
</svg></span><span class="username">&nbsp;ArchikiPrasad</span></a>

</div>
</div>

        </footer>
      </div>
    </div>

    

  </body>
</html>

